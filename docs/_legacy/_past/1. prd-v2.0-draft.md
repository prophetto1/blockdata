# PRD — MD-Annotate v2.0

**Purpose:** The envisioned final product — features, users, value, success criteria
**Time Orientation:** Future (end state)
**Status:** Draft (v2.0). Once finalized, this document is LOCKED and never modified.

Companion documents:
- `docs/product-defining-v2.0/2. prd-technical-spec-v3.0-draft.md` (architecture + canonical output contract)
- `docs/product-defining-v2.0/immutable-fields.md` (immutable field definitions + pairing rules)
- `docs/product-defining-v2.0/blocks.md` (block types + hybrid extraction requirement)
- `docs/product-defining-v2.0/Defining User-Defined Schemas.md` (use case illustrations)

---

## Section 1: Product Vision

### 1.1 One-Sentence Definition

> MD-Annotate is a document intelligence platform that decomposes uploaded documents into ordered, typed blocks and lets users apply custom extraction schemas so that AI workers produce structured, per-block metadata at any scale — from a single manuscript to a 28,000-document corpus.

### 1.2 Core Belief

> We believe that extracting structured knowledge from documents should not require custom ML pipelines, manual reading at scale, or forcing an entire document into a single AI session. If the document can be split into independent units with stable identities, then AI can process each unit against a consistent standard — and the results compose back into something more useful than the original document alone.

### 1.3 Success Vision

> When MD-Annotate succeeds, a legal researcher uploads 28,000 Supreme Court opinions, defines five extraction schemas (paragraph classification, entity extraction, citation graph, rule extraction, topic tagging), and has per-paragraph structured data across the entire corpus — with every extracted field traceable to a specific paragraph in a specific opinion. A general counsel uploads a 45-page contract, applies a clause review schema, and gets an obligation register, a risk heat map, and a deadline tracker — each item linked to the clause that created it. A writer uploads a 39,000-word manuscript, applies a prose editing schema embedding Strunk's 18 rules, and gets a revised manuscript, a narrative flow assessment, and a keyword map — produced paragraph by paragraph at consistent quality, then reassembled in reading order.

---

## Section 2: Problem

### 2.1 The Pain

- **Long documents exceed AI context windows.** A 39,000-word manuscript cannot be processed consistently in a single AI session. The AI shortcuts, loses context, applies standards inconsistently, or skips sections partway through. The user has tried and failed.

- **Structured metadata exists at the document level but not at the paragraph level.** A legal database tells you *Marbury v. Madison* is about judicial review (issue area code 9), decided 6-0, authored by Marshall. It does not tell you which paragraph articulates the holding, which paragraphs cite which cases, or what legal principle is stated in paragraph 12. The gap between document-level metadata and paragraph-level understanding is where all the analytical value lives — and no existing tool fills it.

- **Manual extraction does not scale.** Extracting five metadata fields from 420,000 paragraphs across 28,000 documents is thousands of hours of expert reading. Reviewing a 45-page contract clause-by-clause for obligations, risk flags, cross-references, and deadlines takes 6–10 hours per contract. The work is not conceptually hard — it is voluminous.

### 2.2 Who Feels It

| Audience | How They Experience the Pain |
|:--|:--|
| Legal researchers | Have large corpora (thousands of cases, statutes, filings) with rich case-level databases but no paragraph-level structured data. Cannot answer "which paragraph states the holding?" or "where is this case cited and for what principle?" at scale. |
| Academic writers | Have long manuscripts (30,000–100,000 words) that need consistent paragraph-level editing, assessment, or metadata extraction. No single AI session can maintain quality across the full document. |
| Legal professionals (general counsel, contract reviewers) | Receive long commercial contracts (30–80 pages) that need clause-level analysis: obligations, risk flags, defined terms, cross-references, deadlines. Manual review is slow and inconsistent across reviewers. |
| Analysts and knowledge engineers | Need to build knowledge graphs, compliance registers, or structured datasets from unstructured document collections. Existing tools provide document-level classification but not paragraph-level entity/relation extraction with provenance. |

### 2.3 Current Workarounds

| Workaround | Limitation |
|:--|:--|
| Single-session AI (paste entire document into ChatGPT/Claude) | Context window limits mean the AI loses coherence partway through. Quality degrades silently. No stable identifiers for cross-referencing results back to source text. |
| Manual reading + spreadsheet | Scales linearly with human hours. Inconsistent across readers. No provenance — hard to trace an extracted field back to the exact paragraph that generated it. |
| Custom NLP/ML pipelines | Require data science expertise to build and maintain. Inflexible — changing the extraction schema means retraining or rewriting code. Not accessible to domain experts (lawyers, writers, researchers) who know what to extract but cannot build pipelines. |
| Document-level AI classification | Produces one label per document (e.g., "this contract is high risk"). Cannot locate *which clause* creates the risk or *which paragraph* states the holding. The granularity is wrong. |

---

## Section 3: Users

### Persona 1: Legal Researcher

**Who they are:** An academic or institutional researcher working with a large legal corpus — hundreds to tens of thousands of judicial opinions, statutes, or regulatory filings.

**Their goal:** Extract paragraph-level structured metadata (rhetorical function, citations with treatment, legal principles, entities, topic classification) from every document in the corpus to build a knowledge graph, run quantitative analysis, or support legal scholarship.

**Their context:** Already has case-level structured databases (e.g., SCDB, Shepard's, Fowler scores) but needs the paragraph layer that connects document-level metadata to specific text. Works with `.md` files or formats convertible to structured blocks. Comfortable defining extraction schemas in JSON. May run multiple schemas over the same corpus iteratively.

**Success for them:** Every paragraph in every document has stable, structured metadata — traceable to a specific `block_uid` — that can be queried, joined to existing databases, and composed into a knowledge graph. A new schema can be applied to the same corpus without re-uploading or re-processing the immutable blocks.

---

### Persona 2: Academic Writer

**Who they are:** A writer or editor working on a long-form document — a legal-academic manuscript, a dissertation, a policy report, or a book chapter — typically 20,000–100,000 words.

**Their goal:** Apply a consistent editorial or analytical standard to every paragraph of the manuscript independently, then reassemble the results in reading order. Tasks include prose editing against a style guide, narrative flow assessment, keyword extraction, or metadata labeling.

**Their context:** The manuscript is already in `.md` format (or convertible). The writer knows the editorial standard they want applied (e.g., Strunk's 18 rules, a house style guide) but cannot get consistent AI application across the full document in one session. Needs the revised text, editorial notes, and analytical summaries to be paragraph-aligned for review.

**Success for them:** The writer gets a JSONL export where each paragraph has been processed against the same schema with the same reference material. They can extract a revised manuscript (concatenating `revised_block` in order), a narrative sentence chain (reading `narrative_summary` in order), and a keyword map — all from one run, without manually splitting or reassembling anything.

---

### Persona 3: Contract Reviewer

**Who they are:** A general counsel, in-house lawyer, or contract manager who reviews commercial agreements — Master Service Agreements, SaaS agreements, licensing contracts, leases — before signing.

**Their goal:** Extract clause-level metadata from a long contract: obligations (who must do what, by when), risk flags (liability caps, indemnification triggers, auto-renewal traps), defined terms usage, internal cross-references, and deadlines.

**Their context:** Receives contracts as Word documents (`.docx`) or PDFs. The contract is 30–80 pages with 100–300 clauses. Needs to review multiple contracts per week. Each review currently takes 6–10 hours of senior associate time. The reviewer knows what to look for but needs a tool that applies the same checklist to every clause consistently.

**Success for them:** The reviewer uploads a DOCX contract, applies a clause review schema, and gets five aggregated outputs: an obligation register (grouped by party), a risk heat map (sorted by severity), a defined terms usage map, a cross-reference graph, and a deadline tracker. Each item traces back to the specific clause and page that generated it. A second, deeper schema (e.g., for indemnification analysis) can be applied to the same blocks without re-uploading.

---

## Section 4: Product Capabilities

### Core Capabilities

1. **Multi-Format Document Upload**
   - Users can upload documents in any supported format — Markdown, DOCX, PDF, PPTX, HTML, XLSX, CSV, images, and others — and the platform accepts and stores them with a deterministic source identity.

2. **Automatic Block Decomposition**
   - Users can upload a document and receive an ordered inventory of typed blocks (paragraphs, headings, list items, tables, footnotes, etc.) without manual splitting. Markdown files are parsed via mdast; all other formats are parsed via Docling. Each block has a stable identity, a normalized type, a provenance pointer back into the parsed representation, and the original text preserved verbatim.

3. **Custom Schema Creation**
   - Users can define extraction schemas as JSON artifacts specifying what metadata to extract from each block. Schemas can include field definitions, instructions, reference material (e.g., a style guide, a signal taxonomy, a defined terms list), and any other context the user wants AI workers to have. The platform stores schemas without interpreting their internal structure beyond requiring a valid JSON object with a `schema_ref` string.

4. **Schema Binding and AI Processing**
   - Users can bind a schema to a document (creating a "run") and have AI workers process each block independently against that schema. Workers receive the block's text and the full schema artifact. Processing scales with worker count — blocks are independent units of work with no collisions.

5. **In-Platform Block Viewer (Online Mode)**
   - Users can view blocks and their schema overlays directly in the web interface as a row-by-row table — without downloading anything. The left columns show immutable block data (index, type, content preview); the right columns show the schema overlay fields, dynamically shaped by the schema applied. A run selector switches which schema's results are displayed. As AI workers process blocks, rows update from pending to complete in real time. This is the primary experience for single-document use cases (contract review, manuscript editing, result inspection before schema refinement).

### Extended Capabilities

6. **Multi-Schema Overlay**
   - Users can apply multiple schemas to the same document without re-uploading or re-processing the immutable blocks. Each schema produces its own overlay and its own column set in the block viewer. The same blocks support N schemas via separate runs — the immutable substrate is stable throughout.

7. **Corpus-Scale Processing**
   - Users can upload thousands of documents, bind a schema to all of them, and process the resulting blocks concurrently. The platform handles the decomposition, work distribution, and export assembly. A 28,000-document corpus with 5 schemas produces ~2.1 million block evaluations — all parallelizable.

8. **Structured JSONL Export (Export Mode)**
   - Users can export results as JSONL (one JSON object per line, one line per block, ordered by reading position). Each exported record contains the immutable block envelope alongside the user-defined schema overlay. The export serializes the same data the block viewer displays — same join, different output format. This is the primary experience for corpus-scale downstream pipelines (DuckDB, Neo4j, custom scripts).

9. **Iterative Schema Refinement**
    - Users can run a broad schema, examine the results in the block viewer, design a deeper or more targeted schema, and run it on the same blocks. Each run produces a new overlay visible in the viewer and exportable as JSONL. The user refines their extraction strategy without re-ingesting anything.

---

## Section 5: Use Cases

*(Distilled from the detailed use case illustrations in `docs/product-defining-v2.0/Defining User-Defined Schemas.md`. The companion document contains full schema artifacts, concrete JSON exports, and extended discussion for each use case.)*

### Use Case 1: Close Reading of a Legal Corpus

**Scenario:** A legal researcher has 28,000 US Supreme Court majority opinions (~420,000 paragraphs). Existing databases provide 55 case-level metadata columns but zero paragraph-level structured data. The researcher needs five fields extracted from every paragraph: rhetorical function, precedents cited, legal principle, key entities, and reasoning type.

**Journey:**
1. Researcher uploads 28,000 `.md` files (one per opinion).
2. Platform decomposes each into paragraph-level blocks via mdast (~420,000 blocks total).
3. Researcher creates a schema (`scotus_close_reading_v1`) defining the five fields with instructions and enum values.
4. Researcher binds the schema to all documents (28,000 runs).
5. AI workers process 420,000 blocks concurrently.
6. Researcher exports JSONL — each line is one paragraph with immutable metadata + five filled fields.

**Outcome:** Every paragraph in every opinion has stable, structured metadata. The researcher can query across the corpus ("show every paragraph classified as `holding` across all 28,000 opinions") and join block-level extractions to existing case-level databases via deterministic identifiers. A second schema can be applied later without re-uploading.

---

### Use Case 2: Editing a 39,000-Word Manuscript

**Scenario:** A writer has a legal-academic manuscript (~540 blocks) that needs paragraph-level prose editing against Strunk's 18 rules, narrative flow assessment, and keyword extraction. No single AI session can maintain consistent quality across 39,000 words.

**Journey:**
1. Writer uploads the manuscript (`.md`).
2. Platform decomposes into 540 blocks (490 paragraphs, 45 headings, etc.).
3. Writer creates a schema (`prose_edit_and_assess_v1`) with three fields — `revised_block`, `narrative_summary`, `key_terms` — and embeds the full 18-rule Strunk reference as `reference_material`.
4. Writer binds the schema and processes all blocks.
5. Writer exports JSONL.

**Outcome:** From one export, the writer extracts three outputs by reading different fields in order: (1) a revised manuscript (concatenating `revised_block`), (2) a narrative sentence chain for flow assessment (reading `narrative_summary` in sequence), and (3) a keyword map grouped by section. The 39,000-word manuscript is no harder than a 500-word essay — only the number of blocks changes.

---

### Use Case 3: Building a Knowledge Graph from a Legal Corpus

**Scenario:** The same legal researcher from Use Case 1 wants to build a paragraph-addressable knowledge graph with typed nodes (cases, principles, entities, statutes), typed edges (cites, follows, overrules, applies), and full provenance. The researcher already has substantial case-level infrastructure (29K cases in SCDB, 5.7M Shepard's citation edges, Fowler authority scores, Martin-Quinn ideology scores).

**Journey:**
1. Same 420,000 blocks from Use Case 1 (already ingested — no re-upload needed).
2. Researcher creates five layered schemas: structural classification, entity extraction, citation graph, rule/holding extraction, topic tagging.
3. Five schemas bound to 28,000 documents = 140,000 runs = ~2.1 million block evaluations.
4. AI workers process concurrently across all schemas.
5. Researcher exports five JSONL streams per document.
6. Post-export pipeline assembles the knowledge graph: deduplicate entities into nodes, resolve citations into edges, attach topic labels, compute cross-case links.

**Outcome:** A paragraph-addressable citation knowledge graph over the entire SCOTUS corpus — every node and edge traceable to a specific paragraph in a specific opinion. Block-level extractions join back to existing case-level databases via deterministic identifiers (the opinion's SHA-256 hash maps to `source_uid`, case IDs join to SCDB/Fowler/Shepard's). The KG assembly pipeline is downstream of the platform — the platform's job ends at structured JSONL export.

---

### Use Case 4: Reviewing a Commercial Contract (Non-Markdown Format)

**Scenario:** A general counsel receives a 45-page Master Service Agreement as a `.docx` file. The contract has ~214 blocks (headings, clauses, list items, tables). The general counsel needs obligations, risk flags, defined terms, cross-references, and deadlines extracted from every clause.

**Journey:**
1. General counsel uploads the `.docx` file.
2. Platform routes it through the Docling conversion pipeline (not mdast) and decomposes into 214 blocks with `docling_json_pointer` locators (including page numbers).
3. General counsel creates a schema (`contract_clause_review_v1`) with six fields and embeds the MSA's 40 defined terms as `reference_material`.
4. General counsel binds the schema and processes all blocks.
5. General counsel exports JSONL.

**Outcome:** Five aggregated outputs from one export: (1) obligation register grouped by party, (2) risk heat map sorted by severity, (3) defined terms usage index, (4) cross-reference graph between sections, (5) deadline and notice period tracker. Each item traces to a specific clause and page. A second, deeper schema (e.g., for indemnification analysis) can be run on the same blocks.

---

## Section 6: Success Criteria

### 6.1 Quantitative Metrics

| Metric | Target | Rationale |
|:--|:--|:--|
| Supported upload formats | All formats listed in the `source_type` enum (md, docx, pdf, pptx, html, xlsx, csv, image, and others) | The platform must handle the two ingestion tracks: mdast for Markdown, Docling for everything else. |
| Block decomposition accuracy | Every block in the exported inventory has a valid `block_type`, a correct `block_locator` pointing back to the parsed representation, and `block_content` matching the source text at that location | The immutable substrate must be trustworthy — downstream schema work depends on it. |
| Schema processing throughput | Linear scaling with worker count — doubling workers halves wall-clock time | Blocks are independent units of work. The architecture must not introduce bottlenecks that break this property. |
| Export conformance | 100% of exported JSONL records conform to the canonical two-key shape (`immutable` + `user_defined`) with all required fields present and all enum values drawn from the defined lists | The export is the contract. Non-conforming exports are bugs. |
| Multi-schema independence | Applying schema B to a document that already has schema A attached does not modify schema A's overlay or the immutable blocks | Multi-schema is a first-class requirement. Schemas must not interfere with each other. |

### 6.2 Qualitative Signals

- **Schema expressiveness:** Domain experts (lawyers, researchers, writers) can author schemas in JSON that capture their extraction needs — including instructions, reference material, and field definitions — without needing to write code or understand the platform's internals.
- **Provenance trust:** Every extracted field in the export traces to a specific block with a stable identity, a locator pointing into the parsed representation, and the original text preserved verbatim. A user can always answer "where did this come from?" by following the provenance chain.
- **Iterative refinement:** Users run a broad schema, examine results, create a more targeted schema, and run it on the same blocks — without re-uploading or waiting for re-ingestion. The feedback loop between schema design and result inspection is fast.

### 6.3 Definition of Done

The product is complete when:

- [ ] Users can upload documents in Markdown and at least three non-Markdown formats (DOCX, PDF, and one other) and receive a block inventory with correct types, locators, and content
- [ ] The block inventory conforms to the immutable field contract: deterministic `source_uid`, `conv_uid`, and `block_uid`; correct pairing rules between parsing tool, representation type, and locator type
- [ ] Users can create, store, and retrieve schema artifacts as opaque JSON (validated: is JSON object + has `schema_ref` string)
- [ ] Users can bind a schema to a document, creating a run that generates pending overlay rows for every block
- [ ] AI workers can claim and process blocks atomically, writing filled `user_defined.data` per block
- [ ] Users can export JSONL conforming to the canonical output contract (two top-level keys, all fields present, ordered by `block_index`)
- [ ] Multiple schemas can be attached to the same document with independent runs and independent exports
- [ ] One schema can be applied across multiple documents in a corpus
- [ ] The platform provides a web interface with: document upload, schema management, run creation, and export download

---

## Section 7: Platform Boundary

The platform's core responsibility is the block infrastructure: ingest documents, decompose into immutable blocks, bind user-defined schemas, distribute to AI workers, and export structured JSONL. Everything beyond that boundary is achievable through API integrations with external services — not excluded from the product's reach, but not built into the platform itself.

### Platform-Native (What the Platform Builds)

The platform owns the pipeline from upload to structured export:

- Document upload and storage (with deterministic source identity)
- Block decomposition via mdast (Markdown) and Docling (all other formats)
- Schema storage and validation (opaque JSON + `schema_ref`)
- Run creation, work distribution, and overlay persistence
- Canonical JSONL export (the contract)
- Web interface for upload, schema management, run creation, and export

### Achieved Through API Integration (What External Services Provide)

The platform's structured JSONL export and deterministic identifiers (`source_uid`, `conv_uid`, `block_uid`, `schema_uid`) create clean integration points. Each of the following is a downstream or adjacent service consuming or feeding the platform via API:

1. **Knowledge graph assembly** — Entity deduplication, edge resolution, graph storage, and query APIs are handled by external graph services (Neo4j, Amazon Neptune, or a custom pipeline over DuckDB). The platform provides the structured JSONL; the graph service consumes it. The join keys (`block_uid`, `normalized_us_cite`, `schema_ref`) are stable and deterministic — integration is mechanical.

2. **AI worker backends** — The platform distributes blocks to workers but does not host or train models. Any LLM API (OpenAI, Anthropic, Google, open-source endpoints) can serve as the worker backend. Model selection, versioning, and cost optimization are handled by the external provider. The platform's contract with the worker is: receive `block_content` + schema artifact, return filled `user_defined.data`.

3. **Advanced schema validation** — The platform validates opaque JSON + `schema_ref`. Richer validation (JSON Schema enforcement, field type checking, enum constraint verification) can be handled by an external validator service called during schema upload — or by the AI worker itself at processing time. The platform's minimal validation keeps schemas flexible; external services add strictness where users want it.

4. **Analytics and visualization** — Dashboard views, charting, statistical analysis over exported data are handled by external analytics platforms (DuckDB, BigQuery, Tableau, custom frontends). The JSONL export is the interface — structured, ordered, and joinable.

5. **Collaboration and workflow** — Multi-user review workflows (e.g., "assign this schema run to reviewer B for QA"), commenting, and approval chains are handled by external project management or workflow tools. The platform's API exposes runs, exports, and schema metadata for integration.

6. **Document source integrations** — Pulling documents from content management systems, cloud storage (S3, Google Drive, SharePoint), or case management databases for upload. The platform accepts files; the source integration fetches them.

### Architectural Principle

The platform is a pipeline layer, not a monolith. Its value is the block infrastructure — deterministic decomposition, stable identities, opaque schema overlay, structured export. External services plug in at well-defined API boundaries:

- **Upstream:** document sources feed the platform
- **Processing:** any LLM API serves as the AI worker
- **Downstream:** graph DBs, analytics platforms, visualization tools, and workflow systems consume the export

This is the same architectural pattern as the Learning Commons KG: a data infrastructure (the KG repo with JSONL exports + API) loosely coupled to consuming applications (the evaluator repo with LLM chains). The platform provides the structured data layer; external services build on it.

### Future Platform-Native Extensions (Not This Version)

- **Block-groups** — Grouping sequential blocks (e.g., groups of 10, or section-delimited groups) and applying schemas at the group level. This is a derived entity over the existing block inventory. Group identity would be deterministic: `group_uid` derived from `(conv_uid, grouping_strategy, range)`. Block-groups are a natural extension once per-block processing is stable, but they are a separate export mode — not an addition to the per-block export contract.

- **Schema marketplace** — A registry where users can browse, share, or fork schemas created by others. The schema system supports this architecturally (schemas are self-contained JSON artifacts with stable `schema_uid` identifiers), but the sharing UX is not in scope for v2.0.

- **Diff and comparison views** — Side-by-side comparison of two schema runs over the same blocks (e.g., comparing `scotus_structural_v1` vs. `scotus_structural_v2` outputs). Useful for schema refinement but not required for initial release.

---

## Writing Guidelines Compliance Note

This PRD follows the template's writing guidelines:
1. **No implementation language** — Capabilities describe what users can do, not how the system is built internally.
2. **User-centric framing** — Every capability starts with "Users can..."
3. **Concrete examples** — Use cases reference specific documents, schemas, and outputs from the companion illustrations document.
4. **Testable criteria** — Success metrics in Section 6 are observable and verifiable.
5. **Vision, not roadmap** — This document describes the destination. The technical specification (`2. prd-technical-spec-v3.0-draft.md`) describes the architecture. Implementation sequencing is separate.
