# PRD v2.0 (Draft) - Block Inventory + User-Defined Schemas

Status: Draft (reconstructed from existing docs after migration drift)
Last updated: 2026-02-07

This document is the **future-oriented** PRD (Document 1). It describes the intended product end state in **user terms** (no architecture / implementation details).

Primary source material used to reconstruct intent:
- `docs/core/ref/product-vision.md`
- `docs/core/core-docs/1. prd-v1.0.md`
- `docs/_drifted/prd-v4.md` (for use cases and user-visible flows; technical details intentionally excluded here)

---

## Section 1: Product Vision

### 1.1 One-Sentence Definition

MD-Annotate (working name) is a document intelligence platform that turns unstructured documents into a **permanent, identity-bearing block inventory** that users can annotate with **swappable schemas** to enable parallel AI processing, multi-lens analysis, and downstream exports.

### 1.2 Core Belief

The "block" (paragraph-level unit) should be a first-class citizen with permanent identity and provenance - not a disposable chunk on the way to a vector store.

Same blocks, infinite lenses.

### 1.3 Success Vision

When this product succeeds:
- A user can upload a 500-1000 page document, apply a strict editing or extraction schema, and complete consistent work via parallel AI workers without losing provenance.
- A legal researcher can ingest a large corpus (e.g., thousands of cases), run multiple independent schemas (citations, holdings, authority signals) without re-ingesting, and export structured datasets where every datapoint links back to the exact source block.
- Teams stop rebuilding one-off pipelines per objective; the platform becomes the reusable hub that converts documents into durable, composable units.

---

## Section 2: Problem

### 2.1 The Pain

- Single-AI context limits: large documents cannot be processed coherently by one model instance.
- Re-processing waste: to run two different analyses (e.g., citations and holdings), current tools typically require re-running pipelines from scratch.
- Missing provenance: many document-to-RAG workflows produce disposable chunks with weak traceability back to the exact paragraph/source location.
- Fragmented ecosystem: IDP tools, human annotation tools, and RAG tooling each solve slices, but there is no durable "hub" artifact that supports many lenses over the same content.

### 2.2 Who Feels It

| Audience | How They Experience the Pain |
|----------|------------------------------|
| Legal researchers / analysts | Need multiple extractions over large corpora with strict provenance; current workflows are repeated, fragile, and hard to validate. |
| Technical writers / editors | Need consistent editing across huge documents; cannot parallelize reliably across many AI workers without a stable unit of work. |
| Knowledge engineers | Need entity/relation extraction with provenance; document-level extraction loses granularity and reusability. |
| AI builders | Need retrieval-ready units with rich metadata and deterministic IDs to support filtered search, auditing, and recomposition. |

### 2.3 Current Workarounds

| Workaround | Limitation |
|------------|------------|
| Manual copy/paste + spreadsheets | Does not scale; no automation; provenance is brittle and hard to audit. |
| One-off scripts per extraction objective | Repeated ingestion and re-parsing; no shared identity model; hard to reproduce results. |
| RAG chunking + vector store | Chunks are typically disposable; provenance and reuse across multiple schemas is weak. |
| Traditional IDP / OCR tools | Often doc-level, not block-centric; harder to run many distinct "lenses" with consistent join keys. |

---

## Section 3: Users

### Persona 1: Legal Researcher / Analyst (Primary)

**Who they are:** Power user working with large corpora of cases/contracts/policies.

**Their goal:** Extract structured signals (citations, holdings, clause types, risk flags) and build datasets/graphs with provenance.

**Their context:** Large datasets, repeated analyses, need to validate outputs and trace every claim to source text.

**Success for them:** They can run multiple schemas across the same ingested corpus, export datasets, and audit every field back to the exact source block.

---

### Persona 2: Technical Writer / Editor

**Who they are:** Writer/editor working on long manuals, papers, or books with strict style standards.

**Their goal:** Apply consistent editing rules (e.g., style guide enforcement, clarity rewrites) across massive documents.

**Their context:** Large documents exceed context windows; they want parallel AI work that remains consistent and reviewable.

**Success for them:** They can run a style/editing schema across blocks, review changes with provenance, and reconstruct a revised document.

---

### Persona 3: Knowledge Engineer

**Who they are:** Builder of structured knowledge bases and graphs from text sources.

**Their goal:** Extract entities/relations/triples and load them into downstream systems with citations to source text.

**Their context:** Needs repeatable extraction and the ability to re-run improved schemas without re-ingestion.

**Success for them:** They can generate graph-ready exports where each node/edge cites the originating block(s).

---

### Persona 4: AI Builder / Data Engineer

**Who they are:** Engineer building retrieval, analytics, and evaluation pipelines.

**Their goal:** Create deterministic, reusable units for indexing, filtering, and evaluation.

**Their context:** Needs stable identifiers, reliable exports, and the ability to stitch results back to source.

**Success for them:** They can integrate outputs into vector DBs/BI/ETL while preserving a clean provenance chain.

---

## Section 4: Product Capabilities

### Core Capabilities

1. **Deterministic block inventory**
   - Users can upload a document and receive a deterministic list of blocks (e.g., paragraphs/headings/tables) in reading order with stable IDs and provenance metadata.

2. **User-defined schemas ("lenses")**
   - Users can create/upload schemas that define what per-block outputs should be produced (fields, structure, rules).
   - Users can attach **many schemas** to the same document (not just one).
   - Users can see, at a glance, which schemas are attached to a document and navigate to each schema's outputs.
   - Users can open a schema and see all documents that use it.

3. **Parallelizable processing**
   - Users can process blocks in parallel (multiple AI workers) while maintaining consistency because each worker operates on stable, well-scoped block units.

4. **Multiple schemas on the same blocks**
   - Users can apply multiple independent schemas to the same ingested content without re-ingestion or re-parsing.

5. **Portable exports with provenance**
   - Users can export results to a portable format (e.g., JSONL) where every output field is traceable back to the block(s) that produced it.

### Extended Capabilities

6. **Document reconstruction**
   - Users can reconstruct a transformed document by applying schema outputs (e.g., rewrites/edits) back onto blocks and exporting a revised document.

7. **Downstream routing**
   - Users can route exports into downstream systems (knowledge graphs, analytics pipelines, vector indexes) using stable block/document identifiers as join keys.

8. **Corpus-scale workflows**
   - Users can run schemas over many documents, aggregate results, and compare outputs across a corpus.

---

## Section 5: Use Cases

### Use Case 1: Corpus-Wide Structured Extraction

**Scenario:** A user uploads a corpus and extracts consistent structured fields across all documents.

**Journey:**
1. User uploads N documents.
2. System produces a deterministic block inventory for each document.
3. User selects/uploads an extraction schema defining target fields.
4. System processes blocks in parallel under the schema.
5. User exports structured output for analysis (dataset/graph/matrix).

**Outcome:** A structured dataset derived from an unstructured corpus, with block-level provenance.

---

### Use Case 2: Distributed Document Processing (Editing / Review)

**Scenario:** A user uploads a single massive document and applies an editing schema (e.g., style rules).

**Journey:**
1. User uploads a large document that exceeds typical AI context limits.
2. System decomposes it into blocks.
3. User selects a processing schema defining per-block operations (rule checks, rewrite candidates, flags).
4. Multiple AI workers process blocks concurrently.
5. User reviews results and reconstructs a revised document.

**Outcome:** A long document is processed consistently with parallel AI work, without losing traceability.

---

### Use Case 3: Knowledge Graph Construction

**Scenario:** A user extracts entities and relationships from documents to populate a graph database.

**Journey:**
1. User uploads source documents.
2. System produces a deterministic block inventory.
3. User selects a KG extraction schema (entities, relations, triples).
4. System annotates blocks under that schema.
5. User exports graph-ready data with citations to source blocks.

**Outcome:** A knowledge graph ingest artifact with strong provenance.

---

## Section 6: Success Criteria

### 6.1 Quantitative Metrics

| Metric | Target | Rationale |
|--------|--------|-----------|
| Determinism | 100% identical block IDs and ordering for the same input + settings | Enables reuse, caching, and auditable provenance. |
| Ingest throughput | TBD | Drives usability on large corpora. |
| Export correctness | 0 invalid exports in smoke tests | Exports are the portable contract to downstream systems. |
| Parallel processing scale | TBD (blocks/minute, workers) | Core differentiator vs single-context workflows. |

### 6.2 Qualitative Signals

- Users describe the platform as a "hub" that eliminates rebuilding pipelines per objective.
- Users trust outputs because every field traces back to a specific block with stable identity.
- Users can comfortably run multiple distinct schemas on the same document without fear of drift.

### 6.3 Definition of Done

The product is complete when:

- [ ] Users can upload documents and obtain a deterministic block inventory with stable IDs and provenance.
- [ ] Users can apply at least one user-defined schema to produce per-block structured outputs.
- [ ] Users can export a portable artifact with provenance suitable for downstream pipelines.
- [ ] Users can apply multiple schemas to the same ingested content without re-ingestion.
- [ ] Users can reconstruct a revised document from schema outputs (when the schema includes edits).

---

## Section 7: Out of Scope

### Will Not Include

1. **A general-purpose note-taking app** - This is not a replacement for editors; it is a block inventory + schema lens platform.
2. **"Only a vector store chunker"** - Embedding/indexing is downstream; the primary artifact is the durable block inventory with provenance.
3. **Manual-first human annotation tooling** - The core value is schema-driven, parallelizable machine-assisted processing; manual workflows are secondary.

### Future Considerations (Not This Version)

- Schema marketplaces / sharing workflows across orgs.
- Advanced reconstruction targets (layout-perfect export) beyond the initial reconstruction path.
- Rich evaluation / benchmarking suite for schema outputs across corpora.
